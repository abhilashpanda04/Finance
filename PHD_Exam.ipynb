{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing all the required libaries for visualization and EDA process \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading train & test data\n",
    "# test_data = pd.read_csv('/content/drive/MyDrive/PHD Exam/test_data.csv')\n",
    "train_data = pd.read_csv('/content/drive/MyDrive/PHD Exam/train_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading other data also\n",
    "foot_fall = pd.read_csv(\"/content/drive/MyDrive/PHD Exam/foot_fall.csv\")\n",
    "dis_freature = pd.read_csv(\"/content/drive/MyDrive/PHD Exam/discount_features.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/content/drive/MyDrive/PHD Exam/city_dict.json') as f:\n",
    "  city_dict = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-Manipulation-and-creating-base-data-after-merging-other-data-file\">Data Manipulation and creating base data after merging other data file<a class=\"anchor-link\" href=\"#Data-Manipulation-and-creating-base-data-after-merging-other-data-file\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul>\n",
    "<li>Quick check all the data</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train data\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample= train_data.groupby(['year','month','day','city','medicine']).apply(lambda x: x.sample(frac=0.20))\n",
    "df_sample.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test data\n",
    "test_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# foot fall data\n",
    "foot_fall.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# discount feature\n",
    "dis_freature.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# City Dictonery\n",
    "city_dict = pd.DataFrame.from_dict(city_dict,orient='index')\n",
    "city_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "city_dict = city_dict.reset_index()\n",
    "city_dict = city_dict.rename(columns={city_dict.columns[0]:'city',city_dict.columns[1]:'city_name'})\n",
    "city_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Changing city date-type so i can mapped wiht base data\n",
    "city_dict['city'] = city_dict[\"city\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><em>Creating based data joining all required data</em></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating date columns with year,month,day\n",
    "train_data[\"date\"] = pd.to_datetime(train_data[['year', 'month', 'day']])\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample= train_data.groupby(['date','city']).apply(lambda x: x.sample(frac=0.20))\n",
    "df_sample.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_sample.reset_index(drop=True)\n",
    "df_sample.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample.to_csv(\"/content/drive/MyDrive/PHD Exam/df_sample.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Creating-final-data-after-mapping-all-data\">Creating final data after mapping all data<a class=\"anchor-link\" href=\"#Creating-final-data-after-mapping-all-data\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping city from city_dict\n",
    "final_train_df = pd.merge(train_data,city_dict,left_on=\"city\",right_on=\"city\",how=\"left\")\n",
    "final_train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Melting footfall data so we can mapped with final_train_df data\n",
    "idcol = \"city\"\n",
    "varcol = np.setdiff1d(foot_fall.columns,idcol)\n",
    "foot_fall_melted = pd.melt(foot_fall,id_vars=idcol,value_vars=varcol)\n",
    "foot_fall_melted[\"variable\"] = foot_fall_melted[\"variable\"].astype('datetime64[ns]')\n",
    "foot_fall_melted = foot_fall_melted.rename(columns={'value':'footfall_count'})\n",
    "foot_fall_melted.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping with base data - \"final_train_df\" based of city and variable\n",
    "final_train_df = pd.merge(final_train_df,foot_fall_melted,left_on=[\"date\",'city_name'],right_on=['variable',\"city\"],how=\"left\")\n",
    "final_train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing duplicte colums\n",
    "final_train_df = final_train_df.drop([\"variable\",'city_y'],axis = 1)\n",
    "final_train_df = final_train_df.rename(columns={'city_x':'city'})\n",
    "final_train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shape of data frame after mergging all nessary data\n",
    "final_train_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Droping all duplicate rows if it exit in dateframe\n",
    "final_train_df = final_train_df.drop_duplicates()\n",
    "final_train_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><em>Key Finding</em></p>\n",
    "<p>1.Base data contain duplicate rows  so removing and processing further</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Descriptive-Statitics\">Descriptive Statitics<a class=\"anchor-link\" href=\"#Descriptive-Statitics\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checking basic information of dataframe\n",
    "final_train_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Univariate analysis of data which give some basic understanding of each and every variable in dataframe\n",
    "final_train_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><em>Key Finding and solution</em></p>\n",
    "<ol>\n",
    "<li>As we can see above sales contain negative value.</li>\n",
    "</ol>\n",
    "<p><em>Solution</em></p>\n",
    "<ol>\n",
    "<li>We can convert negative sales value as zero(0) sales value. </li>\n",
    "<li>We can remove those rows which contain -ve sales value.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#missing data\n",
    "total = final_train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (final_train_df.isnull().sum()/final_train_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><em>Key Finding and solution</em></p>\n",
    "<ol>\n",
    "<li>Footfall_count contain .25% value of missing value.</li>\n",
    "</ol>\n",
    "<p><em>Solution</em></p>\n",
    "<ol>\n",
    "<li>It is very small portion of total values so we can impute missing value or we can remove rows which contain missing values.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unqiue value for few columns of data\n",
    "print(\"unique value in year are :\",final_train_df[\"year\"].unique())\n",
    "print('\\n')\n",
    "print(\"unique value in month are :\",final_train_df[\"month\"].unique())\n",
    "print('\\n')\n",
    "print(\"unique value in city_name :\",final_train_df[\"city_name\"].unique())\n",
    "print('\\n')\n",
    "print(\"unique value in medicine are :\",final_train_df[\"medicine\"].unique())\n",
    "print('\\n')\n",
    "print(\"unique value in day are :\",final_train_df[\"day\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-Visualization\">Data Visualization<a class=\"anchor-link\" href=\"#Data-Visualization\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing all required libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To pass the warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><strong>Note</strong></p>\n",
    "<h2 id=\"Data-is-quite-huge-not-able-to-do-visualization-EDA,-Below-are-few-sample-of-visualization-and-google-colab-is-crashing-very-frequently-and-picking-ranmdom-sample-from-data-and-performing-visualization-EDA\">Data is quite huge not able to do visualization EDA, Below are few sample of visualization and google colab is crashing very frequently and picking ranmdom sample from data and performing visualization EDA<a class=\"anchor-link\" href=\"#Data-is-quite-huge-not-able-to-do-visualization-EDA,-Below-are-few-sample-of-visualization-and-google-colab-is-crashing-very-frequently-and-picking-ranmdom-sample-from-data-and-performing-visualization-EDA\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating two seprate list of categorical and numerical variable \n",
    "cat_variable = ['year', 'month','day','city','city_name']\n",
    "num_variable = ['sales','footfall_count','medicine']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Box plot between Year and sales \n",
    "sns.boxplot(data = final_train_df, x='year', y='sales')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\n",
    "sns.distplot(\n",
    "    final_train_df['sales'], bins=20, hist_kws={\"alpha\": 1}\n",
    ").set(xlabel='year', ylabel='sales');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Box plot for categorical variable\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for var, subplot in zip(cat_variable, ax.flatten()):\n",
    "    sns.boxplot(x=var, y='sales', data=final_train_df, ax=subplot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Random-sampling-from-train-data-and-performing-EDA-process\">Random sampling from train data and performing EDA process<a class=\"anchor-link\" href=\"#Random-sampling-from-train-data-and-performing-EDA-process\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_train_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Randomly selecting 25% of the base data = \"final_train_df\" \n",
    "train_data_random_data = final_train_df.sample(frac=0.25)\n",
    "train_data_random_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_random_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scatterplot between year and sales \n",
    "sns.scatterplot(x=train_data_random_data['year'], y=train_data_random_data['sales'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scatterplot between month and sales \n",
    "sns.scatterplot(x=train_data_random_data['month'], y=train_data_random_data['sales'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scatterplot between city_name and sales\n",
    "k = sns.scatterplot(x=train_data_random_data['city_name'],y=train_data_random_data['sales'])\n",
    "plt.xticks(rotation=45)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scatterplot between day and sales\n",
    "sns.scatterplot(x=train_data_random_data['day'],y=train_data_random_data['sales'])\n",
    "plt.xticks(rotation=45)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><em>Key Finding</em></p>\n",
    "<ol>\n",
    "<li>Year - as we can in scatterplot between sales and year there are some outlier in 2018.</li>\n",
    "<li>Month - scatterplot between sales and month there are some outlier\n",
    "between 4 and 12 months.</li>\n",
    "<li>Days - scatterplot between sales and day there are some outlier\n",
    "between 15 -20 days.</li>\n",
    "<li>city_name - outlier follow same trend in city as we can see delhi has maximum outliear available .</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Working-with-base-model\">Working with base model<a class=\"anchor-link\" href=\"#Working-with-base-model\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing all required libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating date columns with year,month,day\n",
    "train_data[\"date\"] = pd.to_datetime(train_data[['year', 'month', 'day']])\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_data[['date','city','medicine','sales']]\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample= train_data.groupby(['date','city']).apply(lambda x: x.sample(frac=0.20))\n",
    "df_sample.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_sample.reset_index(drop = True)\n",
    "df_sample.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_sample[[\"date\",\"sales\"]].reset_index(drop=True)\n",
    "df_sample.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assigning date as a index and again group by date\n",
    "df_sample= df_sample.groupby(['date'])['sales'].sum()\n",
    "df_sample.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_mean = df_sample.rolling(window=90).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_mean.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"As-we-can-see-there-is-bit-of-trend-in-data\">As we can see there is bit of trend in data<a class=\"anchor-link\" href=\"#As-we-can-see-there-is-bit-of-trend-in-data\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Base-Line-model---Moving-avg\">Base Line model - Moving avg<a class=\"anchor-link\" href=\"#Base-Line-model---Moving-avg\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_df = pd.concat([df_sample,df_sample.shift(1)],axis=1)\n",
    "sales_df = sales_df.reset_index(drop = True)\n",
    "sales_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_df.columns = [\"Actual_sales\",'forecast_sales']\n",
    "sales_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing model library\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculating error\n",
    "sales_error = mean_squared_error(sales_df[\"Actual_sales\"].iloc[1:],sales_df[\"forecast_sales\"].iloc[1:])\n",
    "sales_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.sqrt(sales_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"It-is-naive-model,-which-describe-that-we-have-a-error-of-+--59302.50-which-is-near-or-std-deviation-and-advance-models-errro-should-be-less-then-this.-It-is-very-basic-base-line-model\">It is naive model, which describe that we have a error of +- 59302.50 which is near or std deviation and advance models errro should be less then this. It is very basic base line model<a class=\"anchor-link\" href=\"#It-is-naive-model,-which-describe-that-we-have-a-error-of-+--59302.50-which-is-near-or-std-deviation-and-advance-models-errro-should-be-less-then-this.-It-is-very-basic-base-line-model\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"ARIMA-=-Autoregressive-(p)-Integrated-(d)-Moving-Avg-(q)\">ARIMA = Autoregressive (p) Integrated (d) Moving Avg (q)<a class=\"anchor-link\" href=\"#ARIMA-=-Autoregressive-(p)-Integrated-(d)-Moving-Avg-(q)\">¶</a></h1><h5 id=\"Moving-Avg-=-Smooting-part(It-include-how-to-remove-noise-and-error-part-from-data)\">Moving Avg = Smooting part(It include how to remove noise and error part from data)<a class=\"anchor-link\" href=\"#Moving-Avg-=-Smooting-part(It-include-how-to-remove-noise-and-error-part-from-data)\">¶</a></h5><h5 id=\"Autoregressive-=-How-current-value-is-co-related-with-previous-value-and-previous-value-is-the-best-reflexion-of-the-current-value\">Autoregressive = How current value is co-related with previous value and previous value is the best reflexion of the current value<a class=\"anchor-link\" href=\"#Autoregressive-=-How-current-value-is-co-related-with-previous-value-and-previous-value-is-the-best-reflexion-of-the-current-value\">¶</a></h5><h5 id=\"Integrated-=-Order-of-difference-(Series-should-be-stationary)\">Integrated = Order of difference (Series should be stationary)<a class=\"anchor-link\" href=\"#Integrated-=-Order-of-difference-(Series-should-be-stationary)\">¶</a></h5><h6 id=\"For-p-and-q-we-have-ACF(Auto-corealation)-curve-and-PACF(Partial-auto-corealtion)-curve.\">For p and q we have ACF(Auto corealation) curve and PACF(Partial auto corealtion) curve.<a class=\"anchor-link\" href=\"#For-p-and-q-we-have-ACF(Auto-corealation)-curve-and-PACF(Partial-auto-corealtion)-curve.\">¶</a></h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for ACF and PACF\n",
    "import statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ploting plot_acf is to identify parameter Q\n",
    "# ARIMA require 3 parameter = p,d,q\n",
    "plot_acf(df_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Building-another-model\">Building another model<a class=\"anchor-link\" href=\"#Building-another-model\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gain groping by city and medicine and sum by sales\n",
    "dataset= train_data.pivot_table(index=[\"city\",\"medicine\"],values='sales',columns=[\"date\"],aggfunc=\"sum\")\n",
    "dataset.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# picking same sample from data\n",
    "dataset=dataset.iloc[:,1000:1269]\n",
    "dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reseting index of data\n",
    "dataset.reset_index(inplace=True)\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading test data\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/PHD Exam/test_data.csv')\n",
    "# Mapping test data with sample data\n",
    "dataset=pd.merge(test_data,dataset,on=[\"medicine\",\"city\"],how=\"left\")\n",
    "# Droping unessary columns from data\n",
    "dataset.drop(['id','year','month','day','medicine','city'],inplace = True, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols =dataset.columns\n",
    "cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.iloc[:5000]\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filing na value with lower value from respective row in dataframe \n",
    "for i in cols:\n",
    "  dataset.astype(\"float16\")\n",
    "  dataset.fillna('bfill',inplace=True)\n",
    "return dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Starting-model-building-process\">Starting model building process<a class=\"anchor-link\" href=\"#Starting-model-building-process\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X we will keep all columns execpt the last one \n",
    "X_train = np.expand_dims(dataset.values[:,:-1],axis = 2)\n",
    "# the last column is our label\n",
    "y_train = dataset.values[:,-1:]\n",
    "\n",
    "# for test we keep all the columns execpt the first one\n",
    "X_test = np.expand_dims(dataset.values[:,1:],axis = 2)\n",
    "\n",
    "# lets have a look on the shape \n",
    "print(X_train.shape,y_train.shape,X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing libraries required for our model\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # our defining our model \n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units = 64,input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model_lstm.add(Dropout(0.4))\n",
    "model_lstm.add(Dense(1))\n",
    "\n",
    "model_lstm.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "model_lstm.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#history_lstm = model_lstm.fit(X_train,y_train,batch_size = 4096,epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot the loss curves for training\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_lstm.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.legend(loc='best', shadow=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLP for Time Series Forecasting (Multilayer Perceptron )\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_mlp.add(Dropout(0.4))\n",
    "model_mlp.add(Dense(1))\n",
    "\n",
    "model_mlp.compile(loss='mse', optimizer=adam, metrics = ['mean_squared_error'])\n",
    "model_mlp.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_mlp = model_mlp.fit(X_train,y_train,batch_size = 4096,epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history_mlp.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.legend(loc='best', shadow=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #CNN for Time Series Forecasting\n",
    "adam = optimizers.Adam()\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss='mse', optimizer=adam)\n",
    "model_cnn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_history = model_cnn.fit(X_train, y_train, epochs=10, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn_history.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.legend(loc='best', shadow=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CNN-LSTM for Time Series Forecasting\n",
    "\n",
    "#Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features]\n",
    "\n",
    "subsequences = 3\n",
    "timesteps = X_train.shape[1]//subsequences\n",
    "X_train_series_sub = X_train.reshape((X_train.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_train_series_sub.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adam = optimizers.Adam()\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(64, activation='relu',return_sequences=True))\n",
    "model_cnn_lstm.add(Dropout(0.4))\n",
    "model_cnn_lstm.add(LSTM(64, activation='relu',return_sequences=True))\n",
    "model_cnn_lstm.add(Dropout(0.4))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss='mse', optimizer=adam)\n",
    "model_cnn_lstm.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, y_train, epochs=20, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn_lstm_history.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.legend(loc='best', shadow=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating submission file \n",
    "submission_pfs = model_cnn_lstm.predict(X_test)\n",
    "we will keep every value between 0 and 20\n",
    "submission_pfs = submission_pfs.clip(0,20)\n",
    "# creating dataframe with required columns \n",
    "submission = pd.DataFrame({'ID':test_data['ID'],'item_cnt_month':submission_pfs.ravel()})\n",
    "# creating csv file from dataframe\n",
    "submission.to_csv('sub_pfs.csv',index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features\n",
    "subsequences = 3\n",
    "timesteps = X_test.shape[1]//subsequences\n",
    "X_test_series_sub = X_test.reshape((X_test.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_test_series_sub.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = pd.DataFrame({'ID':test['id'],'sales':submission_pfs.ravel()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission.to_csv('sample_submission.csv',index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
